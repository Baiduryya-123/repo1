{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1150616,"sourceType":"datasetVersion","datasetId":649927},{"sourceId":6602304,"sourceType":"datasetVersion","datasetId":3809235},{"sourceId":6876576,"sourceType":"datasetVersion","datasetId":3951369},{"sourceId":6892933,"sourceType":"datasetVersion","datasetId":3959785},{"sourceId":6892955,"sourceType":"datasetVersion","datasetId":3959799},{"sourceId":7130181,"sourceType":"datasetVersion","datasetId":4113674},{"sourceId":7135853,"sourceType":"datasetVersion","datasetId":4117571}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf   \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array, load_img\nfrom tensorflow.keras.applications import ResNet50,Xception\nfrom tensorflow.keras.models import Sequential,Model   \nfrom tensorflow.keras.layers import Dense,Input, GlobalAveragePooling2D, Dense, Dropout, multiply, Reshape, Conv2D, Activation, Add, Dropout, GlobalAveragePooling2D, BatchNormalization, Multiply,MaxPooling2D,Concatenate,Conv2D, UpSampling2D, Cropping2D,Lambda\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler,ModelCheckpoint\n\nimport shutil\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.metrics import Accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:15:33.272785Z","iopub.execute_input":"2025-05-12T13:15:33.273052Z","iopub.status.idle":"2025-05-12T13:15:45.624718Z","shell.execute_reply.started":"2025-05-12T13:15:33.273032Z","shell.execute_reply":"2025-05-12T13:15:45.624148Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 13:15:34.582473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747055734.762994      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747055734.816454      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt    \nimport matplotlib.image as mpimg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:16:55.341222Z","iopub.execute_input":"2025-05-12T13:16:55.342244Z","iopub.status.idle":"2025-05-12T13:16:55.345561Z","shell.execute_reply.started":"2025-05-12T13:16:55.342216Z","shell.execute_reply":"2025-05-12T13:16:55.344760Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set the image size and batch size for training\nbatch_size =16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:16:57.450235Z","iopub.execute_input":"2025-05-12T13:16:57.450824Z","iopub.status.idle":"2025-05-12T13:16:57.454195Z","shell.execute_reply.started":"2025-05-12T13:16:57.450799Z","shell.execute_reply":"2025-05-12T13:16:57.453493Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define input shape \ninput_shape = (400,400,3)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:16:58.772442Z","iopub.execute_input":"2025-05-12T13:16:58.772918Z","iopub.status.idle":"2025-05-12T13:16:58.776542Z","shell.execute_reply.started":"2025-05-12T13:16:58.772896Z","shell.execute_reply":"2025-05-12T13:16:58.775834Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset_dir='/kaggle/input/isic-2017-preprocessed-augmented/content/Linear_Exact_Aug/Train'   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:16:59.722756Z","iopub.execute_input":"2025-05-12T13:16:59.723443Z","iopub.status.idle":"2025-05-12T13:16:59.726736Z","shell.execute_reply.started":"2025-05-12T13:16:59.723416Z","shell.execute_reply":"2025-05-12T13:16:59.725922Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"Valid_dataset_dir='/kaggle/input/isic-2017-preprocessed-augmented/content/Linear_Exact_Aug/Valid'   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:11.080171Z","iopub.execute_input":"2025-05-12T13:17:11.080471Z","iopub.status.idle":"2025-05-12T13:17:11.084019Z","shell.execute_reply.started":"2025-05-12T13:17:11.080448Z","shell.execute_reply":"2025-05-12T13:17:11.083059Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Data augmentation and normalization for training\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0 / 255.0,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:12.343032Z","iopub.execute_input":"2025-05-12T13:17:12.343634Z","iopub.status.idle":"2025-05-12T13:17:12.346910Z","shell.execute_reply.started":"2025-05-12T13:17:12.343611Z","shell.execute_reply":"2025-05-12T13:17:12.346231Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the training dataset with data augmentation\ntrain_generator = train_datagen.flow_from_directory(\n    train_dataset_dir,   \n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:27.096221Z","iopub.execute_input":"2025-05-12T13:17:27.096752Z","iopub.status.idle":"2025-05-12T13:17:30.789098Z","shell.execute_reply.started":"2025-05-12T13:17:27.096725Z","shell.execute_reply":"2025-05-12T13:17:30.788589Z"}},"outputs":[{"name":"stdout","text":"Found 14018 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"test_dataset_dir = '/kaggle/input/isic-2017-preprocessed-augmented/content/Linear_Exact_Aug/Test' ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:39.831816Z","iopub.execute_input":"2025-05-12T13:17:39.832080Z","iopub.status.idle":"2025-05-12T13:17:39.835828Z","shell.execute_reply.started":"2025-05-12T13:17:39.832060Z","shell.execute_reply":"2025-05-12T13:17:39.835125Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntest_data_generator = ImageDataGenerator(\n    rescale=1.0/255.0,  # Normalize pixel values to the range [0, 1]\n    # Add any other preprocessing options if needed\n)\n\ntest_generator = test_data_generator.flow_from_directory(\n    test_dataset_dir,    \n    target_size=(400,400),  # Adjust to match your model's input size\n    batch_size=16,           # Adjust batch size as needed   \n    class_mode='categorical',  # If you have class labels\n    shuffle=False              # Do not shuffle test data\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:42.485571Z","iopub.execute_input":"2025-05-12T13:17:42.485844Z","iopub.status.idle":"2025-05-12T13:17:43.084900Z","shell.execute_reply.started":"2025-05-12T13:17:42.485823Z","shell.execute_reply":"2025-05-12T13:17:43.084382Z"}},"outputs":[{"name":"stdout","text":"Found 600 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Data normalization for validation and testing\nval_datagen = ImageDataGenerator(rescale=1.0 / 255.0)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:45.639416Z","iopub.execute_input":"2025-05-12T13:17:45.639946Z","iopub.status.idle":"2025-05-12T13:17:45.643370Z","shell.execute_reply.started":"2025-05-12T13:17:45.639926Z","shell.execute_reply":"2025-05-12T13:17:45.642656Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Load the validation dataset\nval_generator = val_datagen.flow_from_directory(\n    Valid_dataset_dir,   \n    target_size=input_shape[:2],\n    batch_size=batch_size,    \n    class_mode='categorical'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:48.096873Z","iopub.execute_input":"2025-05-12T13:17:48.097446Z","iopub.status.idle":"2025-05-12T13:17:48.196381Z","shell.execute_reply.started":"2025-05-12T13:17:48.097423Z","shell.execute_reply":"2025-05-12T13:17:48.195859Z"}},"outputs":[{"name":"stdout","text":"Found 150 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Create an input layer\ninput_layer = Input(shape=input_shape)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:57.514670Z","iopub.execute_input":"2025-05-12T13:17:57.515342Z","iopub.status.idle":"2025-05-12T13:17:57.519839Z","shell.execute_reply.started":"2025-05-12T13:17:57.515321Z","shell.execute_reply":"2025-05-12T13:17:57.519242Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import tensorflow_hub as hub    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:17:59.583903Z","iopub.execute_input":"2025-05-12T13:17:59.584550Z","iopub.status.idle":"2025-05-12T13:18:01.061092Z","shell.execute_reply.started":"2025-05-12T13:17:59.584524Z","shell.execute_reply":"2025-05-12T13:18:01.060577Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#bit_l_url = \"https://tfhub.dev/google/bit/m-r101x1/1\"   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:18:01.688476Z","iopub.execute_input":"2025-05-12T13:18:01.689012Z","iopub.status.idle":"2025-05-12T13:18:01.692845Z","shell.execute_reply.started":"2025-05-12T13:18:01.688988Z","shell.execute_reply":"2025-05-12T13:18:01.692067Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Load the BiT L model from TensorFlow Hub\n#bit_l_model = hub.KerasLayer(bit_l_url, trainable=True)\n#bit_l_output = bit_l_model(input_layer)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:20:17.577039Z","iopub.execute_input":"2025-05-12T13:20:17.577637Z","iopub.status.idle":"2025-05-12T13:21:49.399610Z","shell.execute_reply.started":"2025-05-12T13:20:17.577612Z","shell.execute_reply":"2025-05-12T13:21:49.398533Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1999351010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the BiT L model from TensorFlow Hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbit_l_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_l_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbit_l_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbit_l_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                      lambda: f(training=False))\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       parameters.append(\n\u001b[0;32m--> 583\u001b[0;31m           _make_validated_mono_param(name, arg, poly_parameter.kind,\n\u001b[0m\u001b[1;32m    584\u001b[0m                                      \u001b[0mtype_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                                      poly_parameter.type_constraint))\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    520\u001b[0m ) -> Parameter:\n\u001b[1;32m    521\u001b[0m   \u001b[0;34m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m   \u001b[0mmono_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpoly_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmono_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoly_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_np_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTENSOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;34m\"A KerasTensor is symbolic: it's a placeholder for a shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;34m\"an a dtype. It doesn't have any actual numerical value. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'keras_layer_1' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'keras_layer_1' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 400, 400, 3), dtype=float32, sparse=False, name=keras_tensor>\n  • training=None"],"ename":"ValueError","evalue":"Exception encountered when calling layer 'keras_layer_1' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'keras_layer_1' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 400, 400, 3), dtype=float32, sparse=False, name=keras_tensor>\n  • training=None","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"#bit_l_output_reshaped = Reshape((1, 1, -1))(bit_l_output)  # Reshape to match the expected shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:21:58.034374Z","iopub.execute_input":"2025-05-12T13:21:58.034871Z","iopub.status.idle":"2025-05-12T13:21:58.049485Z","shell.execute_reply.started":"2025-05-12T13:21:58.034848Z","shell.execute_reply":"2025-05-12T13:21:58.048547Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2265309444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbit_l_output_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit_l_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to match the expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'bit_l_output' is not defined"],"ename":"NameError","evalue":"name 'bit_l_output' is not defined","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# Load the Xception model without top classification layers\nxception_base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\nfor layer in xception_base_model.layers:\n    layer.trainable = True     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:22:01.002515Z","iopub.execute_input":"2025-05-12T13:22:01.002781Z","iopub.status.idle":"2025-05-12T13:22:07.597577Z","shell.execute_reply.started":"2025-05-12T13:22:01.002761Z","shell.execute_reply":"2025-05-12T13:22:07.596777Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Connect the input layer to the base models\nxception_features = xception_base_model(input_layer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:22:48.211767Z","iopub.execute_input":"2025-05-12T13:22:48.212533Z","iopub.status.idle":"2025-05-12T13:22:48.217278Z","shell.execute_reply.started":"2025-05-12T13:22:48.212507Z","shell.execute_reply":"2025-05-12T13:22:48.216539Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Load the Xception model without top classification layers\nresnet50_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\nfor layer in xception_base_model.layers:\n    layer.trainable = True     \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:23:55.426016Z","iopub.execute_input":"2025-05-12T13:23:55.426798Z","iopub.status.idle":"2025-05-12T13:24:00.585179Z","shell.execute_reply.started":"2025-05-12T13:23:55.426770Z","shell.execute_reply":"2025-05-12T13:24:00.584628Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Connect the input layer to the base models\nresnet50_features = resnet50_base_model(input_layer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:26:58.125761Z","iopub.execute_input":"2025-05-12T13:26:58.126320Z","iopub.status.idle":"2025-05-12T13:26:58.131041Z","shell.execute_reply.started":"2025-05-12T13:26:58.126296Z","shell.execute_reply":"2025-05-12T13:26:58.130303Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"CCA Feature Fusion     ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Dense, Concatenate\n\nclass CCALikeFusion(Layer):\n    def __init__(self, latent_dim=100, fusion_type='concat', **kwargs):\n        super(CCALikeFusion, self).__init__(**kwargs)\n        self.latent_dim = latent_dim\n        self.fusion_type = fusion_type\n\n        # Learnable projections\n        self.proj_x = Dense(latent_dim, use_bias=False, name=\"proj_x\")\n        self.proj_y = Dense(latent_dim, use_bias=False, name=\"proj_y\")\n\n        if self.fusion_type == 'concat':\n            self.concat = Concatenate()\n\n    def call(self, inputs):\n        x, y = inputs  # x and y are feature maps (e.g., from Xception and ResNet50)\n\n        # Project to latent space\n        x_proj = self.proj_x(x)\n        y_proj = self.proj_y(y)\n\n        if self.fusion_type == 'concat':\n            return self.concat([x_proj, y_proj])\n        elif self.fusion_type == 'average':\n            return (x_proj + y_proj) / 2.0\n        else:\n            raise ValueError(\"fusion_type must be 'concat' or 'average'\")\n\n    def get_config(self):\n        config = super(CCALikeFusion, self).get_config()\n        config.update({\n            \"latent_dim\": self.latent_dim,\n            \"fusion_type\": self.fusion_type\n        })\n        return config\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:41:10.251918Z","iopub.execute_input":"2025-05-12T13:41:10.252503Z","iopub.status.idle":"2025-05-12T13:41:10.258556Z","shell.execute_reply.started":"2025-05-12T13:41:10.252480Z","shell.execute_reply":"2025-05-12T13:41:10.257777Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"combined_features = CCALikeFusion(latent_dim=100, fusion_type='concat')([xception_features, resnet50_features])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:43:51.910895Z","iopub.execute_input":"2025-05-12T13:43:51.911186Z","iopub.status.idle":"2025-05-12T13:43:51.963124Z","shell.execute_reply.started":"2025-05-12T13:43:51.911165Z","shell.execute_reply":"2025-05-12T13:43:51.962587Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"#combined_features =multiply([xception_features,bit_l_output_reshaped])   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply Squeeze-and-Excitation block\ndef se_block(input_tensor):   \n    num_channels = input_tensor.shape[-1]\n    \n    # Squeeze operation (Global Average Pooling)\n    squeeze = GlobalAveragePooling2D()(input_tensor)\n    squeeze = Reshape((1, 1, num_channels))(squeeze)\n    \n    # Excitation operation (Fully connected layers)\n    excitation = Dense(num_channels // 16, activation='relu')(squeeze)\n    excitation = Dense(num_channels, activation='sigmoid')(excitation)\n    \n    # Scale the input feature maps\n    scaled_features = multiply([input_tensor, excitation])\n    \n    return scaled_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:00.080229Z","iopub.execute_input":"2025-05-12T13:44:00.080514Z","iopub.status.idle":"2025-05-12T13:44:00.085282Z","shell.execute_reply.started":"2025-05-12T13:44:00.080494Z","shell.execute_reply":"2025-05-12T13:44:00.084675Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"se_output = se_block(combined_features)\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:03.285740Z","iopub.execute_input":"2025-05-12T13:44:03.286425Z","iopub.status.idle":"2025-05-12T13:44:03.314493Z","shell.execute_reply.started":"2025-05-12T13:44:03.286404Z","shell.execute_reply":"2025-05-12T13:44:03.313583Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Add a classification head to the combined features\nfrom tensorflow.keras.regularizers import l2\nx = tf.keras.layers.GlobalAveragePooling2D()(se_output)\nx = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)\nx = Dropout(0.1)(x)\nx = tf.keras.layers.Dense(2048, activation='relu')(x)\nx = tf.keras.layers.Dense(512, activation='selu',kernel_regularizer=l2(0.02))(x)\nx = BatchNormalization()(x)\noutput = tf.keras.layers.Dense(3, activation='softmax')(x)  # 7output classes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:05.486287Z","iopub.execute_input":"2025-05-12T13:44:05.486867Z","iopub.status.idle":"2025-05-12T13:44:05.532455Z","shell.execute_reply.started":"2025-05-12T13:44:05.486844Z","shell.execute_reply":"2025-05-12T13:44:05.531898Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Create the ensemble model\nmodel = Model(inputs=input_layer, outputs=output)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:08.396010Z","iopub.execute_input":"2025-05-12T13:44:08.396638Z","iopub.status.idle":"2025-05-12T13:44:08.402640Z","shell.execute_reply.started":"2025-05-12T13:44:08.396612Z","shell.execute_reply":"2025-05-12T13:44:08.401903Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\n\n# Define your model here\nmodel = model\n\n# Specify the file path where you want to save the model architecture image\nimage_path = 'model_architecture.png'\n\n# Plot the model architecture and save it as an image\nplot_model(model, to_file=image_path, show_shapes=True, show_layer_names=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:12.716297Z","iopub.execute_input":"2025-05-12T13:44:12.717015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:21.593132Z","iopub.execute_input":"2025-05-12T13:44:21.593382Z","iopub.status.idle":"2025-05-12T13:44:21.621388Z","shell.execute_reply.started":"2025-05-12T13:44:21.593344Z","shell.execute_reply":"2025-05-12T13:44:21.620863Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ xception (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │     \u001b[38;5;34m20,861,480\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │     \u001b[38;5;34m23,587,712\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cca_like_fusion           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m200\u001b[0m)    │        \u001b[38;5;34m409,600\u001b[0m │ xception[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mCCALikeFusion\u001b[0m)           │                        │                │ resnet50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ cca_like_fusion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m12\u001b[0m)       │          \u001b[38;5;34m2,412\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │          \u001b[38;5;34m2,600\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m200\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ cca_like_fusion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n│                           │                        │                │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │        \u001b[38;5;34m205,824\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │      \u001b[38;5;34m2,099,200\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │      \u001b[38;5;34m1,049,088\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │          \u001b[38;5;34m2,048\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │          \u001b[38;5;34m1,539\u001b[0m │ batch_normalization_4… │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cca_like_fusion           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">409,600</span> │ xception[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CCALikeFusion</span>)           │                        │                │ resnet50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cca_like_fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,412</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cca_like_fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n│                           │                        │                │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">205,824</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │ batch_normalization_4… │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m48,221,503\u001b[0m (183.95 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,221,503</span> (183.95 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m48,112,831\u001b[0m (183.54 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,112,831</span> (183.54 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m108,672\u001b[0m (424.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,672</span> (424.50 KB)\n</pre>\n"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.losses import categorical_crossentropy\n\ndef combined_loss(y_true, y_pred, alpha=0.2, categorical_weight=0.5):\n    # Compute the categorical cross-entropy loss\n    cat_loss = categorical_crossentropy(y_true, y_pred)\n\n    # Reshape the inputs to get anchor, positive, and negative examples\n    anchor = tf.reshape(y_pred[:, 0], shape=(-1, 1))\n    positive = tf.reshape(y_pred[:, 1], shape=(-1, 1))\n    negative = tf.reshape(y_pred[:, 2], shape=(-1, 1))\n\n    # Compute the distance between the anchor and the positive\n    pos_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n    \n    # Compute the distance between the anchor and the negative\n    neg_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n\n    # Compute the triplet loss\n    triplet_basic_loss = pos_distance - neg_distance + alpha\n    triplet_loss = tf.reduce_mean(tf.maximum(triplet_basic_loss, 0.0), axis=0)\n\n    # Compute the combined loss\n    combined_loss = categorical_weight * cat_loss + (1 - categorical_weight) * triplet_loss\n\n    return combined_loss\n\n# Example usage in a Keras model\nmodel.compile(optimizer='adam', loss=combined_loss, metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:30.102039Z","iopub.execute_input":"2025-05-12T13:44:30.102619Z","iopub.status.idle":"2025-05-12T13:44:30.117866Z","shell.execute_reply.started":"2025-05-12T13:44:30.102595Z","shell.execute_reply":"2025-05-12T13:44:30.117249Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Implement learning rate scheduling   \nlr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:44:36.266827Z","iopub.execute_input":"2025-05-12T13:44:36.267436Z","iopub.status.idle":"2025-05-12T13:44:36.270702Z","shell.execute_reply.started":"2025-05-12T13:44:36.267413Z","shell.execute_reply":"2025-05-12T13:44:36.270085Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"\n# Define the model checkpoint callback to save the best weights\nmodel_checkpoint = ModelCheckpoint('ISIC2017_Classification_contour_ensemble5.weights.h5', monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:08.573673Z","iopub.execute_input":"2025-05-12T13:45:08.574215Z","iopub.status.idle":"2025-05-12T13:45:08.578027Z","shell.execute_reply.started":"2025-05-12T13:45:08.574194Z","shell.execute_reply":"2025-05-12T13:45:08.577160Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Implement early stopping\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=30, verbose=1, restore_best_weights=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:11.644487Z","iopub.execute_input":"2025-05-12T13:45:11.645050Z","iopub.status.idle":"2025-05-12T13:45:11.648479Z","shell.execute_reply.started":"2025-05-12T13:45:11.645029Z","shell.execute_reply":"2025-05-12T13:45:11.647627Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Train the model\nepochs =30\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // 32,\n    epochs=epochs,   \n    validation_data=val_generator,\n    validation_steps=val_generator.samples // 32,\n    callbacks=[lr_scheduler,model_checkpoint,early_stopping]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:15.951710Z","iopub.execute_input":"2025-05-12T13:45:15.952252Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1747057601.691009     111 service.cc:148] XLA service 0x7b3c200023d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1747057601.691835     111 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1747057609.537781     111 cuda_dnn.cc:529] Loaded cuDNN version 90300\nE0000 00:00:1747057629.525973     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057629.754910     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057631.072064     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057631.307955     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057632.741902     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057632.933662     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057634.198972     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057634.397836     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057634.817769     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1747057635.041065     111 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1747057663.727940     111 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698ms/step - accuracy: 0.4598 - loss: 6.3202\nEpoch 1: val_accuracy improved from -inf to 0.39062, saving model to ISIC2017_Classification_contour_ensemble5.weights.h5\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 723ms/step - accuracy: 0.4598 - loss: 6.3118 - val_accuracy: 0.3906 - val_loss: 0.6688 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612ms/step - accuracy: 0.5434 - loss: 0.5535\nEpoch 2: val_accuracy improved from 0.39062 to 0.60938, saving model to ISIC2017_Classification_contour_ensemble5.weights.h5\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 619ms/step - accuracy: 0.5434 - loss: 0.5534 - val_accuracy: 0.6094 - val_loss: 0.6101 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m  1/438\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:24\u001b[0m 605ms/step - accuracy: 0.2500 - loss: 0.6061","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3: val_accuracy improved from 0.60938 to 0.68182, saving model to ISIC2017_Classification_contour_ensemble5.weights.h5\n\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.2500 - loss: 0.6061 - val_accuracy: 0.6818 - val_loss: 0.6053 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m250/438\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 612ms/step - accuracy: 0.5297 - loss: 0.5068","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Plot the training and validation accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the training and validation loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')   \nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n#Load the saved weights from the output directory in the executed model architecture .  \nmodel.load_weights('/kaggle/working/ISIC2017_Classification_contour_ensemble5.h5')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_generator)\nprint(f\"Test Loss: {test_loss}\")    \nprint(f\"Test Accuracy: {test_accuracy}\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n#  the true labels and predicted labels for the test dataset\ny_true = test_generator.classes\ny_pred = model.predict(test_generator).argmax(axis=1)\n   \n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n    \n# Plot the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_generator.class_indices.keys())\ndisp.plot(cmap='viridis')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = {0:'melanoma', 1:'nevus', 2:'seborrheic_keratosis'}\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import classification_report\ncm = confusion_matrix(y_true, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]   \nfig, ax = plt.subplots(figsize=(8,6))  \nsns.heatmap(cmn, annot=True, xticklabels=labels.values(), yticklabels=labels.values(),cmap=plt.cm.Blues, fmt='.2f')\nplt.ylabel('Actual Classes')\nplt.xlabel('Predicted Classes')\nplt.show(block=False)    \n   \n# Generate the classification report\nreport = classification_report(y_true, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)    \nprint(report)     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nprint(\"Precision: \"+ str(precision_score(y_true, y_pred, average='weighted')))\nprint(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\nprint(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))\nf1 = f1_score(y_true, y_pred, average='weighted')\nprint(\"F1 Score: \" + str(f1))\n     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Precision: \"+ str(precision_score(y_true, y_pred, average='macro')))\nprint(\"Recall: \"+ str(recall_score(y_true, y_pred, average='macro')))\nprint(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))\nf1 = f1_score(y_true, y_pred, average='macro')\nprint(\"F1 Score: \" + str(f1))\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Precision: \"+ str(precision_score(y_true, y_pred, average='micro')))\nprint(\"Recall: \"+ str(recall_score(y_true, y_pred, average='micro')))\nprint(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))\nf1 = f1_score(y_true, y_pred, average='micro')\nprint(\"F1 Score: \" + str(f1))\n         ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\ny_true = test_generator.classes   \n# Get the probabilities for each class (0 and 1) from the model predictions\ny_prob = model.predict(test_generator)\n\n# Compute the ROC curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(train_generator.num_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n   \n# Plot the ROC curves\nplt.figure()\nfor i in range(train_generator.num_classes):\n    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score    \nprint(\"weighted Roc score: \" + str(roc_auc_score(y_true,y_prob,multi_class='ovr',average='weighted')))\nprint(\"macro Roc score: \" + str(roc_auc_score(y_true,y_prob,multi_class='ovr',average='macro')))    \nprint(\"micro Roc score: \" + str(roc_auc_score(y_true,y_prob,multi_class='ovr',average='micro')))  ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}